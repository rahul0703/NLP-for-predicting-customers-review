{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural  Language Processing is all obout analysing text and this can be book, text doc, html web page, etc. And it is a branch of Machine learning we do some predective analysis on text. Here we analyse the text review and sort the if the review is positive or negative. This will be the general algorith and we can also apply the same to analyse the book and other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let start with algorithem\n",
    "#importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "dataset = pd.read_csv(\"Restaurant_Reviews.tsv\", delimiter = \"\\t\", quoting = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      "Review    1000 non-null object\n",
      "Liked     1000 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                           Wow... Loved this place.      1\n",
      "1                                 Crust is not good.      0\n",
      "2          Not tasty and the texture was just nasty.      0\n",
      "3  Stopped by during the late May bank holiday of...      1\n",
      "4  The selection on the menu was great and so wer...      1\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Liked\n",
      "count  1000.00000\n",
      "mean      0.50000\n",
      "std       0.50025\n",
      "min       0.00000\n",
      "25%       0.00000\n",
      "50%       0.50000\n",
      "75%       1.00000\n",
      "max       1.00000\n"
     ]
    }
   ],
   "source": [
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" cleaning the dataset is like preparing our dataset to apply our machine learning model. The step include:\n",
    "1. Get rid of the punctuations as this create many confusions of the words and difficult to understand.\n",
    "2. steaming the dataset like sorting some words like loved, liked and converting them to ove and like respectively.\n",
    "3. converting all the uppercase letter to lower case letter.\n",
    "4. get reid of the nuumbers, unless they are not relevent.\n",
    "5. we will construct columns for each associated word and will count if it appears in the review or not. this will create more\n",
    "zeros in our columns.\n",
    "6. we will use sparse matrix as most of the part we will be having 0's as most are new words\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here our input to the sub command is \"[^a-zA-Z]\" which means we want everything to remove or sort expect the letter\\nfrom a to z and also capital letters from A to Z, also here zA is not seperated and this create a problem and\\nso to sort that we add a new paramer space '"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning the texts\n",
    "#importing re as its is most used library to clean text.\n",
    "import re\n",
    "review = re.sub(\"[^a-zA-Z]\", \" \",  dataset[\"Review\"][0])\n",
    "#here re.sub sort the text on by the letter from A to Z and get free of all the other punctuation, nubers, etc.\n",
    "\"\"\"here our input to the sub command is \"[^a-zA-Z]\" which means we want everything to remove or sort expect the letter\n",
    "from a to z and also capital letters from A to Z, also here zA is not seperated and this create a problem and\n",
    "so to sort that we add a new paramer space \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow    Loved this place \n"
     ]
    }
   ],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow    loved this place \n"
     ]
    }
   ],
   "source": [
    "# the next process will be to convert all the letter to a lower case letter\n",
    "review = review.lower()\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul007\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now in the next process we will get rid of word which are of no use to us to judge the review like article, prepositions, etc.\n",
    "#we need a library which contain most functions of NLP calles nltk\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow', 'loved', 'this', 'place']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" now we will run a for loop for all the review for the same process we done above for 1st review\"\"\"\n",
    "review =review.split()\n",
    "print(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow', 'loved', 'place']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "review = [word for word in review if not word in set(stopwords.words(\"english\"))]\n",
    "#the set function is used to speedup the algo as it guide fot the set of words in the list\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow', 'love', 'place']\n"
     ]
    }
   ],
   "source": [
    "#now appling stemming. we need to import porterstemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#we will redesign our algorithem as the above process was to make understanding and now we can have a fun algorithem here.\n",
    "ps = PorterStemmer()\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words(\"english\"))]\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow love place\n"
     ]
    }
   ],
   "source": [
    "#we will make a list containg all the words and so we use join command with space as to seperate the words for join in single word.\n",
    "review = \" \".join(review)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the same process we will perform for all 1000 review.\n",
    "#we will make a list for all 1000 reviews names corpus\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review_1 = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review_1 = review_1.lower()\n",
    "    review_1 = review_1.split()\n",
    "    ps = PorterStemmer()\n",
    "    review_1 = [ps.stem(word) for word in review_1 if not word in set(stopwords.words('english'))]\n",
    "    review_1 = ' '.join(review_1)\n",
    "    corpus.append(review_1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow love place',\n",
       " 'crust good',\n",
       " 'tasti textur nasti',\n",
       " 'stop late may bank holiday rick steve recommend love',\n",
       " 'select menu great price',\n",
       " 'get angri want damn pho',\n",
       " 'honeslti tast fresh',\n",
       " 'potato like rubber could tell made ahead time kept warmer',\n",
       " 'fri great',\n",
       " 'great touch']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a bag of word model, appluing the machine learning model and tranforming and predicting.\n",
    "\"\"\" we will make a table containing all the review in one column. And then we choose a specific word and count the \n",
    "number of times the specific word appeared in the review \"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "#we can do use stemming , sorting and many other things in countvectorizer but can be more efficient doing it manually.\n",
    "X = cv.fit_transform(corpus).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1565)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can see that the 1565 words are taken from the reviews and so we add a new parameter in countVectorizer a max_feature.\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1500)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" we will train our model to predict of the review. The most common model used in Natual Language Processing are\n",
    "naive bayes decision tree and random forest. We will use here naive bayes first\"\"\" \n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48 48]\n",
      " [18 86]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "negative add       0.73      0.50      0.59        96\n",
      "positive add       0.64      0.83      0.72       104\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       200\n",
      "   macro avg       0.68      0.66      0.66       200\n",
      "weighted avg       0.68      0.67      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target =[\"negative add\", \"positive add\"]\n",
    "print(classification_report(y_test, y_pred, target_names=target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (48+86)/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy {}\\n\".format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 108]\n",
      " [  0  92]]\n"
     ]
    }
   ],
   "source": [
    "#Now we will train random forest to the dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "rf = RandomForestClassifier(n_estimators=400, min_samples_leaf=0.12,max_depth = 5,  random_state=SEED)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(cm_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "negative add       0.00      0.00      0.00       108\n",
      "positive add       0.46      1.00      0.63        92\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       200\n",
      "   macro avg       0.23      0.50      0.32       200\n",
      "weighted avg       0.21      0.46      0.29       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul007\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target =[\"negative add\", \"positive add\"]\n",
    "print(classification_report(y_test, y_pred_rf, target_names=target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
